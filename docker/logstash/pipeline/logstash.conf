input {
    # Input configuration for processed JSON log data
	http {
        port => 5010
        tags => ["input-json"]
    }

    # Input configuration for raw log lines
	http {
        port => 5020
        tags => ["input-raw"]
    }
}

filter {
    # Splitting input lines (both JSON and raw logs) to support bulk imports
    split {
        field => "message"
        terminator => "\n"
    }

    if "input-json" in [tags] {
        # Applying JSON filter to process the JSON input
        json {
            "source" => "message"
        }
    }

    if "input-raw" in [tags] {
        # Applying Grok filter to parse and denormalize the raw log lines
        grok {
            "match" => {
                "message" => "%{DATA:service} -%{GREEDYDATA}- \[%{HTTPDATE:datetime}\] \"%{DATA:content}\" %{NUMBER:httpcode:int}"
            }
        }
    }

    mutate {
        # Cleaning up extra data populated by http input plugin to reduce the document size
        "remove_field" => [ "message", "event", "http", "url", "user_agent", "host", "tags" ]
    }

    date {
        # Parsing the log entry datetime field and dtranforming it intom the date
        match => [ "datetime", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
}

output {
    # Proxying the processed data into ElsticSearch
	elasticsearch {
		hosts => "elasticsearch:9200"
		index => "logs"
	}
}
