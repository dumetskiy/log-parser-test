input {
    # Input configuration for processed JSON log data
	http {
        port => 5010
        request_headers_target_field => "headers"
    }
}

filter {
    # Splitting input lines (both JSON and raw logs) to support bulk imports
    split {
        field => "message"
        terminator => "\n"
    }

    if "application/json" == [headers][content_type] {
        # Applying JSON filter to process the JSON input
        json {
            source => "message"
        }
    }

    if "application/raw-log" == [headers][content_type] {
        # Applying Grok filter to parse and denormalize the raw log lines
        grok {
            match => {
                message => "%{DATA:service} -%{GREEDYDATA}- \[%{HTTPDATE:datetime}\] \"%{DATA:content}\" %{NUMBER:httpcode:int}"
            }
        }
    }

    mutate {
        # Cleaning up extra data populated by http input plugin to reduce the document size
        "remove_field" => [ "message", "event", "http", "url", "user_agent", "host", "headers", "tags" ]
    }

    date {
        # Parsing the log entry datetime field and dtranforming it intom the date
        match => [ "datetime", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
}

output {
    # Proxying the processed data into ElsticSearch
	elasticsearch {
		hosts => "elasticsearch:9200"
		index => "logs"
	}
}
